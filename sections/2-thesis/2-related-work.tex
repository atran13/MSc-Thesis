\section{Related Work}
\label{sec:related_work}

This section first outlines the findings of existing urban ethnographic studies on signage and gentrification, whereby an expectation was formed on the distinguishing characteristics that should be learned by the model. This is followed by a brief discussion on scene text attribute learning as an initial research direction. Next, the state-of-the-art of scene text detection are given for the task of extracting signage from facades. Finally, the computer vision model used in the study is discussed.

\subsection{Signage and gentrification}
\label{subsec:related_signs}

The current body of research about storefront signage aesthetics in relation to gentrification exists largely in the context of the US. While some studies considered multiple features, namely font type (or typeface), colors, text density, language, and meaning, others analyzed elements individually.

Findings from Brooklyn, New York\cite{trinch_signsays_2017, snajdr_oldschool_2018, snajdr_preserve_2022} showed that non-gentrified signage typically was text-dense and had larger typeface; names that refer to the location, the owner's name, the type of business, products or services; languages other than English; complementary symbols or images; reference to religion, ethnicity, country of origin, and race. On the contrary, gentrified signage had shorter texts, written in smaller font sizes, lower case letters; more cryptic or ambiguous names; languages other than English that shows sophistication and worldliness. In parallel, a study from Cincinnati, Ohio \cite{rahman_signage_2020} found that signage became more homogeneous as neighborhoods gentrified (less variation in colors and typeface).

Next to this, there are studies that focused on individual attributes of signage. A popular topic in this regard is the linguistic landscape of a city or neighborhood. Examples could be found in Seoul, South Korea \cite{hong_linguistic_2020}: in the neighborhood where the majority of the Chinese population in Seoul reside, Korean signage had been gradually replaced by Chinese signage, mirroring both the demographic composition and the socio-economic standing of these groups. Similarly, more English signage had appeared in a commercial neighborhood in Phnom Penh, Cambodia, seemingly displacing French as a second language - a trend observed alongside globalization and gentrification \cite{kasanga_map_2012}. Besides languages, typeface had also been found to be highly correlated with average household income of the corresponding neighborhoods in London \cite{ma_typef_2019}.

All in all, findings showed clear differences between the characteristics of gentrified and non-gentrified signage. It was therefore hypothesized that the same pattern can be found in Amsterdam's signage, and a computer vision model can be used to distinguish and detect gentrification based on signage.

\subsubsection{Scene text attribute learning}

Taking inspiration from the aforementioned studies, where differences were found per feature (font, color, semantic, etc.), an exploration in this direction was done for this study. The goal was to learn these specific attributes and quantify their variations among gentrified and non-gentrified signage. However, given the state of the data at hand - namely, the lack of annotations other than the gentrification label - as well as resources available, this research direction was deemed unsuitable. The state-of-the-art approaches are described below.

\begin{enumerate}
    \item There exist models that output multiple attributes of interest. Examples are NeurTEx \cite{aggarwal_neurtex_2022} (outputs text bounding box, transcript, and typeface), Fontnet \cite{s_fontnet_2021} (color and typeface), and TaCo \cite{nie_taco_2022} (color and typeface). However, these models are meant for text in graphic design and documents. They are not as reliable when it comes to scene text because of the added noise and distortions of natural images.
    \item Another way is to use individual models for each element:
    \begin{itemize}
        \item Font: State-of-the-art models include DeepFont \cite{wang_deepfont_2015}, HENet \cite{chen_henet_2021}. These models perform well on synthetic data (e.g. AdobeVFR \cite{wang_deepfont_2015}, VFR-2420 \cite{chen_large-scale_2014}), but are not as robust when it comes to natural images.
        \item Text transcripts and semantics: State-of-the-art scene text recognition models include MORAN \cite{luo_multi-object_2019}, CRNN \cite{shi_end2end_2015}, and PARSeq \cite{bautista_scene_2022}. For learning semantics, word embedding models such as FastText \cite{bojanowski_enriching_2017} or Word2Vec \cite{mikolov_efficient_2013} could be used on the transcribed text. However, as was found from text detection (discussed in Section \ref{subsec:result1}), some instances were returned as single words or fragments of words, while others were phrases. Transcribing the text and learning semantics from this data would not give consistent results.
        \item Color: Text colors could be learnt via creating a color histogram \cite{srivastava_review_2015}. Instead, by using a convolution neural network (discussed in Section \ref{subsec:result4}), colors could be learned, among many other features.
    \end{itemize}
\end{enumerate}

To account for the lack of ground truth labels, a synthetic scene text dataset could have been created for training, before inference was done on StreetSwipe. Gupta et al. \cite{gupta_synt_2016} offer a method for this that takes into account image segmentation and depth in generating text, with annotations on bounding box, typeface, color, and text transcript as needed. This approach is also non-viable: It requires either mining street view images that have suitable empty spaces to be synthesized on, or using pre-generated images from the authors, which largely include many types of background other than facades. Either method proves impractical - the former is time-consuming, and the latter is a mismatch compared to StreetSwipe.

Not having found a viable approach to learning signage attribute-by-attribute, a more appropriate research direction was devised that involved learning the general visual representation of signage with a convolutional neural network (CNN). Signs would first be extracted using a scene text detection model, and an image classification model would be trained and tested on these signage.

\subsection{Scene text detection}
Scene text detection involves localizing text in natural images. The challenge in this task stems from the complexity of natural images, with text surrounded by objects, varying in sizes, perspectives, orientations, sometimes curved, obstructed, or blurry.

Benchmark datasets for this task include ICDAR 2013 \cite{icdar13} and 2015 \cite{icdar15}, and TotalText \cite{totaltext}. The most widely implemented models are EAST \cite{zhou_east_2017} and CRAFT \cite{baek_character_2019}. CRAFT was the best performing model on ICDAR 2013, and surpassed EAST on ICDAR 2015 and TotalText. By calculating character region scores (localizing characters) and affinity scores between characters (grouping characters into sequences), the model returns bounding box coordinates. CRAFT has better accuracy on curved, long, and non-horizontal text compared to EAST. The model is also multilingual - a necessary feature considering signage in the data at hand are (at least) in Dutch, English, Chinese, Arabic, and Korean. CRAFT was thus the model used in this study for extracting signage from facades.


\subsection{Image classification}

Using an image classification model helps learning features and detecting gentrification in signage. It also enables qualitative analysis into signage aesthetics as they mirror gentrification, via inspecting what the model correctly identifies to be (non-)gentrified (correct classifications), and which characteristics lead to the model failing to differentiate between classes (misclassifications).

The Residual Network (ResNet) \cite{resnet}, with the use of skip connections in its architecture, had enabled deeper networks to learn more efficiently, without the problem of vanishing or exploding gradients. Fine-tuned ResNet18 and ResNet50 were chosen as candidate models for classifying signage, initialized with pre-trained weights from ImageNet. After achieving satisfactory performance, the model's classification outputs were analyzed. 

