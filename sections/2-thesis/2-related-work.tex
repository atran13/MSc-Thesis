\section{Related Work}
\label{sec:related_work}

This section first outlines the findings of current urban ethnographic studies on signage and gentrification, followed by a brief exploration into scene-text attribute learning. Next, the state-of-the-art of scene-text detection are given. Finally, image classification is discussed and the computer vision model to be used in the study is presented.

\subsection{Signage and gentrification}

As aforementioned, the current body of research about storefront signage aesthetics in relation to gentrification exists largely in the context of the US. While some studies took a holistic approach and considered multiple elements, namely font type (or typeface), colors, text density, language, and meaning, others analyzed elements individually.

Findings from Brooklyn, New York\cite{trinch_signsays_2017, snajdr_oldschool_2018, snajdr_preserve_2022} showed that non-gentrified signage - or as the authors called them: \textit{old-school} signage - typically was text-dense and had larger typeface; names that refer to the location, the owner's name, the type of business, products or services; languages other than English; complementary symbols or images; reference to religion, ethnicity, country of origin, and race. On the contrary, gentrified signage - or \textit{distinction-making} signage - had shorter texts, written in smaller font sizes, lower case letters; more cryptic or ambiguous names, sometimes polysemic and with word-plays; languages other than English that shows sophistication and worldliness. In parallel, a study from Cincinnati, Ohio \cite{rahman_signage_2020} found that signage forms and characters became more homogeneous as neighborhoods became gentrified.

Next to this, other studies had also analyzed storefront signage but with their focus on individual attributes. A popular topic within gentrification in this regard is the linguistic landscape of a city or neighborhood. Examples could be found in Seoul, South Korea \cite{hong_linguistic_2020}: in the neighborhood where the majority of the Chinese population in Seoul reside, Korean signage had been gradually replaced by Chinese signage, mirroring both the demographic composition and the social and economic standing of these groups. Similarly, more English signage had appeared in a commercial neighborhood in Phnom Penh, Cambodia, seemingly displacing French as a second language - a trend observed alongside globalization, gentrification, a generational change in attitudes, and education policies \cite{kasanga_map_2012}. Besides languages in signage, typeface had also been found to be highly correlated with average household income of the corresponding neighborhoods in London \cite{ma_typef_2019}.

All in all, findings showed clear differences between the characteristics of gentrified and non-gentrified signage. It was therefore hypothesized that the same pattern can be found in Amsterdam's signage, and thus a computer vision model can be used to distinguish and detect gentrification based on signage.

\subsubsection{Scene-text attribute learning}

Taking inspiration from the studies outlined above, where signage was analyzed in terms of typeface, color, and textual meaning (among other elements), an exploration in this direction was done for the current study. The initial goal was to learn these specific attributes and quantify, on a large scale, what characteristics differentiated gentrified and non-gentrified signage in Amsterdam. The pipeline would involve detecting the text instances from the original images (text detection), classifying the font type, inferring the dominant colors, transcribing the text (text recognition), and learn semantics via word embeddings. However, given the state of the dataset at hand - namely, the lack of annotations other than the gentrification label - as well as resources available, this research direction was ultimately deemed unsuitable. The state-of-the-art approaches are described below.

\begin{enumerate}
    \item End-to-end: These are models that can take an image and output multiple elements of interest. Examples are NeurTEx \cite{aggarwal_neurtex_2022} (outputs text bounding box, transcript, and typeface), Fontnet \cite{s_fontnet_2021} (outputs color and typeface), and TaCo \cite{nie_taco_2022} (outputs color and typeface). However, these models were designed to be applied in the context of graphic design and on documents. They were not reliable when it comes to scene-text because of the added noise and distortions of natural images.
    \item Step-by-step: This approach uses individual models to learn each element:
    \begin{itemize}
        \item Font: State-of-the-art models included DeepFont \cite{wang_deepfont_2015}, HENet \cite{chen_henet_2021}, and benchmark datasets included AdobeVFR \cite{wang_deepfont_2015}, and VFR-2420 \cite{chen_large-scale_2014}. These data were largely synthetic, and accordingly the models performed well on synthetic data, but were not as robust when it comes to natural images. Furthermore, with neither pre-trained models nor ground truth available on StreetSwipe images, training and evaluation proved to be challenging and unreliable.
        \item Text transcripts and semantics: State-of-the-art scene-text recognition models included MORAN \cite{luo_multi-object_2019}, CRNN \cite{shi_end2end_2015}, and PARSeq \cite{bautista_scene_2022}. For learning semantics, word embedding models such as FastText \cite{bojanowski_enriching_2017} or Word2Vec \cite{mikolov_efficient_2013} could be applied on the transcribed text. However, as was found from text detection (discussed in a later section), text instances varied a lot in resolution, and a large part of the text instances were fragmented upon detection (words were broken up). Transcribing the text and learning semantics from this data would not give meaningful and consistent results.
        \item Color: Text colors could be learnt via creating a color histogram \cite{srivastava_review_2015} per image. But instead, by using a convolution neural network (discussed in a later section), colors could be learned as well along with many other features.
    \end{itemize}
\end{enumerate}

To account for the lack of ground truth labels, it was also considered that a synthetic scene-text dataset was created to be used as training and testing data, before inference was done on StreetSwipe. Gupta et al. \cite{gupta_synt_2016} offered a method for this that took into account image segmentation and depth in order to generate text, with annotations on bounding box, typeface, color, and text transcript as needed. An example usage in a closely related study was done by Ma et al. \cite{ma_typef_2019}, in which the authors synthesized training data to recognize typeface on signage. Considering time constraint, however, this approach was non-viable. It required either mining street view images that do not have text to be synthesized on, or using pre-generated images from Gupta et al., which largely included many types of background other than street view of facades. Either method proved impractical - the former is time-consuming, even when discounting for model training and tuning; and the latter is a domain mismatch compared to StreetSwipe.

All in all, a machine learning approach for replicating existing studies was not found. A more appropriate research direction was devised that involved learning a more general visual representation of the signage with a convolutional neural network. Signs would firstly be extracted using a pre-trained text-detection model, and an image classification model would be trained and tested on these signage.

\subsection{Scene-text detection}
Scene-text detection involves identifying and localizing text in natural images - a task fundamental to the current study. The challenge in this task stems from extracting text from complex images, with the text surrounded by other objects, varying in sizes, perspectives, orientations, sometimes curved, obstructed, or blurry.

Benchmark datasets for this task include ICDAR 2013 \cite{icdar13} and 2015 \cite{icdar15}, and TotalText \cite{totaltext}. The most widely implemented models are EAST \cite{zhou_east_2017} and CRAFT \cite{baek_character_2019}. CRAFT was the best performing model on ICDAR 2013, and surpassed EAST on ICDAR 2015 and TotalText. CRAFT is more accurate on curved, long, and non-horizontal text. By calculating character region scores (localizing characters) and affinity scores between characters (grouping characters into sequences), the model returns word-level bounding boxes. The model is also multilingual - a necessary feature considering signage in the dataset at hand are (at least) in Dutch, English, Chinese and Korean. CRAFT was thus the model used in this study for extracting signage.

The pre-trained CRAFT model was implemented via the EasyOCR Python package \cite{noauthor_jaided_nodate}, which offered an ease of implementation and also allowed for processing multiple languages simultaneously.

\subsection{Image classification}
The Residual Network (ResNet) \cite{resnet}, with the use of skip connections in its architecture, had enabled deeper networks to learn more efficiently, without the problem of vanishing or exploding gradients. For the task of learning and classifying signage, fine-tuned ResNet18 and ResNet50 were chosen as candidate models, initialized with pre-trained weights from ImageNet. After achieving satisfactory performance, the best model's predictions were further analyzed. 

Firstly, StreetSwipe's correctly classified signage per class with high classification probability were inspected. This showed the most typical and distinguishing characteristics of gentrified and non-gentrified signage. In comparison with existing studies (as outlined in Section 2.1), conclusions were drawn on the extent to which patterns found elsewhere were similar to our case of Amsterdam, and thus the extent to which the computer vision model could achieve similar results as visual qualitative analyses in past research.

Secondly, the mis-classified StreetSwipe signage were inspected (e.g. non-gentrified signage classified as gentrified, and vice versa). Samples were taken categorized by high and low classification probability. Mis-classifications with high probability were expected to have the same characteristics as correct classifications - showing the extent to which signage alone could determine a gentrified or non-gentrified facade. In other words, if some gentrified signage actually looked more like typical non-gentrified signage, and if there existed a consistent pattern in their appearance, we could then conclude that the perceptual gentrification (hence, the gentrified label) came from other elements than the signage.

Mis-classifications with low probability were expected to belong to more fuzzy areas, constituting of (a combination of) characteristics such that the model was not able to clearly distinguish between the two classes. This was something that previous studies did not report on as their sole focus was to point out definite differences between gentrified and non-gentrified signage.

Finally, the model was tested on an external dataset of other neighborhoods in the city. Given literature on gentrification in Amsterdam, gentrified and non-gentrified neighborhoods were selected, and street view images from these areas were retrieved from a dataset made available by the Civic AI Lab. Gentrified areas include Jordaan \cite{verlaan_hippies_2022}, Oud-West, De Baarjes \cite{rettberg_when_2019}; and non-gentrified areas include Zuidoost and Nieuw-West \cite{pinkster_stickiness_2020}. Images taken from these neighborhoods are labeled accordingly. Thus, when testing the model (trained on visual perception) on this data (labeled per census data), we could tell to which extent the perception was present on a neighborhood level. In other words, given a gentrified neighborhood, to which extent are signage in that neighborhood seen as gentrified? Further, the model's classifications on this data were also visualized. This showed what the model considered as gentrified and non-gentrified, and were this to be in-line with characteristics found in StreetSwipe, this would support the model's generalizability in detecting gentrification.

