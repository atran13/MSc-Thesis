\section{Related Work}
\label{sec:related_work}

This section first outlines the findings of existing urban ethnographic studies on signage and gentrification, whereby an expectation was formed on the distinguishing characteristics that should be learned by the model. This is followed by a brief discussion on scene text attribute learning as an initial research direction. Next, the state-of-the-art of scene text detection are given for the task of extracting signage from facades. Finally, the computer vision model used in the study is discussed.

\subsection{Signage and gentrification}

The current body of research about storefront signage aesthetics in relation to gentrification exists largely in the context of the US. While some studies considered multiple features, namely font type (or typeface), colors, text density, language, and meaning, others analyzed elements individually.

Findings from Brooklyn, New York\cite{trinch_signsays_2017, snajdr_oldschool_2018, snajdr_preserve_2022} showed that non-gentrified signage typically was text-dense and had larger typeface; names that refer to the location, the owner's name, the type of business, products or services; languages other than English; complementary symbols or images; reference to religion, ethnicity, country of origin, and race. On the contrary, gentrified signage had shorter texts, written in smaller font sizes, lower case letters; more cryptic or ambiguous names; languages other than English that shows sophistication and worldliness. In parallel, a study from Cincinnati, Ohio \cite{rahman_signage_2020} found that signage forms and characters became more homogeneous as neighborhoods became gentrified (less variation in colors and typeface).

Next to this, there are studies that focused on individual attributes of signage. A popular topic in this regard is the linguistic landscape of a city or neighborhood. Examples could be found in Seoul, South Korea \cite{hong_linguistic_2020}: in the neighborhood where the majority of the Chinese population in Seoul reside, Korean signage had been gradually replaced by Chinese signage, mirroring both the demographic composition and the social and economic standing of these groups. Similarly, more English signage had appeared in a commercial neighborhood in Phnom Penh, Cambodia, seemingly displacing French as a second language - a trend observed alongside globalization and gentrification \cite{kasanga_map_2012}. Besides languages, typeface had also been found to be highly correlated with average household income of the corresponding neighborhoods in London \cite{ma_typef_2019}.

All in all, findings showed clear differences between the characteristics of gentrified and non-gentrified signage. It was therefore hypothesized that the same pattern can be found in Amsterdam's signage, and thus a computer vision model can be used to distinguish and detect gentrification based on signage.

\subsubsection{Scene text attribute learning}

Taking inspiration from the studies outlined above, where differences were found per feature (font, color, semantic, etc.), an exploration in this direction was done for the current study. The initial goal was to learn these specific attributes and quantify, on a large scale, how they varied among gentrified and non-gentrified signage. The pipeline would involve detecting the text instances from the original images (scene text detection), classifying the font type, inferring the dominant colors, transcribing the text (scene text recognition), and learn semantics via word embeddings. However, given the state of the dataset at hand - namely, the lack of annotations other than the gentrification label - as well as resources available, this research direction was deemed unsuitable. The state-of-the-art approaches are described below.

\begin{enumerate}
    \item End-to-end: These are models that can take an image and output multiple attributes of interest. Examples are NeurTEx \cite{aggarwal_neurtex_2022} (outputs text bounding box, transcript, and typeface), Fontnet \cite{s_fontnet_2021} (color and typeface), and TaCo \cite{nie_taco_2022} (color and typeface). However, these models are meant for text in graphic design and documents. They are not as reliable when it comes to scene text because of the added noise and distortions of natural images.
    \item Step-by-step: This approach uses individual models to learn each element:
    \begin{itemize}
        \item Font: State-of-the-art models include DeepFont \cite{wang_deepfont_2015}, HENet \cite{chen_henet_2021}, and benchmark datasets include AdobeVFR \cite{wang_deepfont_2015}, and VFR-2420 \cite{chen_large-scale_2014}. These datasets were largely synthetic, and accordingly the models perform well on synthetic data, but are not as robust when it comes to natural images. Furthermore, with neither pre-trained models nor ground truth available, training and evaluation proved to be unreliable.
        \item Text transcripts and semantics: State-of-the-art scene text recognition models include MORAN \cite{luo_multi-object_2019}, CRNN \cite{shi_end2end_2015}, and PARSeq \cite{bautista_scene_2022}. For learning semantics, word embedding models such as FastText \cite{bojanowski_enriching_2017} or Word2Vec \cite{mikolov_efficient_2013} could be applied on the transcribed text. However, as was found from text detection (discussed in a later section), some instances were returned as words or fragments of words, while others were phrases. Transcribing the text and learning semantics from this data would not give meaningful and consistent results.
        \item Color: Text colors could be learnt via creating a color histogram \cite{srivastava_review_2015}. Instead, by using a convolution neural network (discussed in a later section), colors could be learned as well along with many other features.
    \end{itemize}
\end{enumerate}

To account for the lack of ground truth labels, it was also considered that a synthetic scene text dataset was created to be used as training and testing data, before inference was done on StreetSwipe. Gupta et al. \cite{gupta_synt_2016} offer a method for this that takes into account image segmentation and depth in order to generate text, with annotations on bounding box, typeface, color, and text transcript as needed. This approach is also non-viable: It requires either mining street view images that do not have text to be synthesized on, or using pre-generated images from Gupta et al., which largely include many types of background other than street view. Either method proves impractical - the former is time-consuming, and the latter is a domain mismatch compared to StreetSwipe.

Not having found a viable approach to learn signage attribute-by-attribute, a more appropriate research direction was devised that involved learning a more general visual representation of signage with a convolutional neural network. StreetSwipe signs would firstly be extracted using a pre-trained scene text detection model, and an image classification model would be trained and tested on these signage.

\subsection{Scene text detection}
Scene text detection involves localizing text in natural images - a task fundamental to the current study. The challenge in this task stems from the complexity of natural images, with text surrounded by other objects, varying in sizes, perspectives, orientations, sometimes curved, obstructed, or blurry.

Benchmark datasets for this task include ICDAR 2013 \cite{icdar13} and 2015 \cite{icdar15}, and TotalText \cite{totaltext}. The most widely implemented models are EAST \cite{zhou_east_2017} and CRAFT \cite{baek_character_2019}. CRAFT was the best performing model on ICDAR 2013, and surpassed EAST on ICDAR 2015 and TotalText. CRAFT is more accurate on curved, long, and non-horizontal text. By calculating character region scores (localizing characters) and affinity scores between characters (grouping characters into sequences), the model returns bounding box coordinates. The model is also multilingual - a necessary feature considering signage in the dataset at hand are (at least) in Dutch, English, Chinese, Arabic, and Korean. CRAFT was thus the model used in this study for extracting signage.


\subsection{Image classification}

Using an image classification model helps learning features and detecting gentrification in signage. It also enables qualitative analysis into what signage look like as they mirror gentrification, via inspecting what the model saw as (non-)gentrified, and which characteristics lead to the model failing to differentiate between classes.

The Residual Network (ResNet) \cite{resnet}, with the use of skip connections in its architecture, had enabled deeper networks to learn more efficiently, without the problem of vanishing or exploding gradients. Fine-tuned ResNet18 and ResNet50 were chosen as candidate models for classifying signage, initialized with pre-trained weights from ImageNet. After achieving satisfactory performance, the model's outputs were analyzed. 

First, StreetSwipe's correctly classified signage per class with high classification probability were inspected. This showed the most distinguishing characteristics of gentrified and non-gentrified signage. In comparison with existing studies (as outlined in Section 2.1), conclusions were drawn on the extent to which the model could achieve similar results as visual qualitative analyses in past research.

Secondly, the misclassified StreetSwipe signage were inspected - this showed characteristics of cases that the model failed to distinguish. Conclusion could then be made on the extent to which signage alone could fully determine (non-)gentrification to a computer vision model. Samples were taken categorized by high and low classification probability, or certainty. Misclassifications with high certainty were expected to have the same characteristics as signage in the class they were assigned to. Misclassifications with low certainty were expected to belong to more fuzzy areas, constituting of (a combination of) characteristics such that the model was not able to clearly distinguish between the two classes.

Finally, the model was tested on an extended dataset of other neighborhoods in the city. Given literature on gentrification in Amsterdam, gentrified and non-gentrified neighborhoods were selected, and street view images from these areas were retrieved from a dataset made available by the Civic AI Lab. Gentrified areas included Jordaan \cite{verlaan_hippies_2022}, Oud-West, De Baarsjes \cite{rettberg_when_2019}; and non-gentrified areas included Zuidoost and Nieuw-West \cite{pinkster_stickiness_2020}. Images taken from these neighborhoods are labeled accordingly. Thus, when testing the model (trained on visual perception) on this data (labeled per census data), we could tell to which extent the perception was present in other parts of the city. In other words, given a gentrified neighborhood, to which extent would signage in that neighborhood be seen as gentrified? Additionally, the model's classifications on this data were also inspected. This showed what the model considered as gentrified and non-gentrified in this data, and were this to be in-line with characteristics found in StreetSwipe, this would support the model's generalizability in detecting gentrification.

