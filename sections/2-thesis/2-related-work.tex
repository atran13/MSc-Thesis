\section{Related Work}
\label{sec:related_work}

This section first outlines the findings of current ethnographic and urban studies on signage and gentrification, followed by a brief exploration into scene-text attribute learning. Next, the state-of-the-art of scene-text detection are given. Finally, image classification is discussed and the computer vision model to be used in the study is presented.

\subsection{Signage and gentrification}

As aforementioned, the current body of research about storefront signage aesthetics in relation to gentrification exists largely in the context of the US. While some studies took a holistic approach and considered multiple elements, namely font type (or typeface), colors, text density, language, and meaning, others analyzed elements individually.

Findings from Brooklyn, New York\cite{trinch_signsays_2017, snajdr_oldschool_2018, snajdr_preserve_2022} show that non-gentrified signage - or as the authors call them: \textit{old-school} signage - typically is text-dense and has larger typeface; names that refer to the location, the owner's name, the type of business, products or services; languages other than English; complementary symbols or images; reference to religion, ethnicity, country of origin, and race. On the contrary, gentrified signage - or \textit{distinction-making} signage - has shorter texts, written in smaller font sizes, lower case letters; more cryptic or ambiguous names, sometimes polysemic and with word-plays; languages other than English that shows sophistication and worldliness. In parallel, a study from Cincinnati, Ohio \cite{rahman_signage_2020} found that signage forms and characters became more homogeneous as neighborhoods became gentrified.

Next to this, other studies have also analyzed storefront signage but with their focus on individual attributes. A popular topic within gentrification in this regard is the linguistic landscape of a city or neighborhood. Examples can be found in Seoul, South Korea \cite{hong_linguistic_2020}: in the neighborhood where the majority of the Chinese population in Seoul reside, Korean signage has been gradually replaced by Chinese signage, mirroring both the demographic composition and the social and economic standing of these groups. Similarly, more English signage has appeared in a commercial neighborhood in Phnom Penh, Cambodia, seemingly displacing French as a second language - a trend observed alongside globalization, gentrification, a generational change in attitudes, and education policies \cite{kasanga_map_2012}. Besides languages in signage, typeface has also been found to be highly correlated with average household income of the corresponding neighborhoods in London \cite{ma_typef_2019}.

All in all, findings show clear differences between the characteristics of gentrified and non-gentrified signage. It was therefore hypothesized that the same pattern can be found in Amsterdam's signage, and thus a computer vision model can be used to distinguish and detect gentrification based on signage.

\subsubsection{Scene-text attribute learning}
Taking inspiration from the insights outlined above, where signage was analyzed in terms of typeface, color, and textual meaning (among other elements), an exploration in this direction was done for the current study. The initial goal was to learn these specific attributes and quantify, on a large scale, what characteristics differentiate gentrified and non-gentrified signage in Amsterdam. The pipeline would involve detecting the text instances from the original images (text detection), classifying the font type, inferring the dominant colors, transcribing the text (text recognition), and learn semantics via word embeddings. However, given the state of the dataset at hand - namely, the lack of annotations other than the gentrification label - as well as resources available, this research direction was ultimately deemed unsuitable. The state-of-the-art approaches are described below.

\begin{enumerate}
    \item End-to-end: These are models that can take an image and output multiple elements of interest. Examples are NeurTEx \cite{aggarwal_neurtex_2022} (outputs text bounding box, transcript, and typeface), Fontnet \cite{s_fontnet_2021} (outputs color and typeface), and TaCo \cite{nie_taco_2022} (outputs color and typeface). However, these models are designed to be applied in the context of graphic design and on documents. They are not reliable when it comes to scene-text because of the added noise and distortions of natural images.
    \item Step-by-step: This approach uses individual models to learn each element:
    \begin{itemize}
        \item Font: State-of-the-art models include DeepFont \cite{wang_deepfont_2015}, HENet \cite{chen_henet_2021}, and benchmark datasets include AdobeVFR \cite{wang_deepfont_2015}, and VFR-2420 \cite{chen_large-scale_2014}. These data are largely synthetic, and accordingly the models perform well on synthetic data, but are not as robust when it comes to natural images. Furthermore, with neither pre-trained models nor ground truth available on StreetSwipe images, training and evaluation proves to be challenging and unreliable.
        \item Text transcripts and semantics: State-of-the-art scene-text recognition models include MORAN \cite{luo_multi-object_2019}, CRNN \cite{shi_end2end_2015}, and PARSeq \cite{bautista_scene_2022}. For learning semantics, word embedding models such as FastText \cite{bojanowski_enriching_2017} or Word2Vec \cite{mikolov_efficient_2013} could be applied on the transcribed text. However, as was found from text detection (discussed in a later section), the text instances from the data were either not meaningful words (a nature of signage text), or fragmented upon detection (words are broken up). Learning semantics from this data would not give meaningful and consistent results.
        \item Color: Text colors as a standalone feature could be learnt via creating a color histogram \cite{srivastava_review_2015} per image. But instead, by using a convolution neural network (discussed in a later section), colors can be learned as well along with many other features.
    \end{itemize}
\end{enumerate}

To account for the lack of ground truth labels, it was also considered that a synthetic scene-text dataset is created to be used as training and testing data, before inference is done on StreetSwipe. Gupta et al. \cite{gupta_synt_2016} offers a method for this that takes into account image segmentation and depth in order to generate text, with annotations on bounding box, typeface, color, and text transcript as needed. An example usage in a closely related study was done by Ma et al. \cite{ma_typef_2019}, in which the authors generated training data to recognize typeface on signage. Considering time constraint, however, this approach was non-viable. It required either mining street view images that do not have text to be synthesized on, or using pre-generated images from Gupta et al., which largely included many types of background other than street view of facades. Either method proved impractical - the former is time-consuming, even when discounting for model training and tuning; and the latter is a domain mismatch compared to StreetSwipe.

All in all, a machine learning approach for replicating existing studies was not found. A more appropriate research direction was devised that involved learning a more general visual representation of the signage with a convolutional neural network. Signs would firstly be extracted using a pre-trained text-detection model, and a classification model would be trained and tested on these signage.

\subsection{Scene-text detection}
Scene-text detection involves identifying and localizing text in natural images - a task fundamental to the current study. The challenge in this task stems from extracting text from complex images, with the text surrounded by other objects, varying in sizes, perspectives, orientations, sometimes curved, obstructed, or blurry.

Benchmark datasets for this task include ICDAR 2013 \cite{icdar13} and 2015 \cite{icdar15}, and TotalText \cite{totaltext}. The most widely implemented models are EAST \cite{zhou_east_2017} and CRAFT \cite{baek_character_2019}. CRAFT is the best performing model on ICDAR 2013, and surpasses EAST on ICDAR 2015 and TotalText. CRAFT is more accurate on curved, long, and non-horizontal text. By calculating character region scores (localizing characters) and affinity scores between characters (grouping characters into sequences), the model returns word-level bounding boxes. The model is also multilingual - a necessary feature considering signage in the dataset at hand are (at least) in Dutch, English, Chinese and Korean. CRAFT is thus the model used in this study for extracting signage.

The pre-trained CRAFT model was implemented via the EasyOCR Python package \cite{noauthor_jaided_nodate}, which offered an ease of implementation and also allowed for processing multiple languages simultaneously.

\subsection{Image classification}
The Residual Network (ResNet) \cite{resnet}, with the use of skip connections in its architecture, has enabled deeper networks to learn more efficiently, without the problem of vanishing or exploding gradients. For the task of learning and classifying signage, fine-tuned ResNet18 and ResNet50 were chosen as candidate models, initialized with pre-trained weights from ImageNet. After achieving satisfactory performance, the best model's predictions were further analyzed. 

Firstly, the correctly classified signage within the top 20 highest probabilities from each class were inspected - this showed the most typical and distinguishing characteristics of gentrified and non-gentrified signage. In comparison with existing studies (as outlined in section 2.1), conclusions were drawn on the extent to which patterns found elsewhere were similar to our case of Amsterdam.

Secondly, the incorrectly classified signage were inspected. These can be considered the more fuzzy cases - something that previous studies did not report on.

Finally, the model was tested on an external dataset of other neighborhoods in the city. Given literature on gentrification in Amsterdam, gentrified and non-gentrified neighborhoods were selected, and street view images from these areas were retrieved from a dataset made available by the Civic AI Lab. Gentrified areas include Jordaan \cite{verlaan_hippies_2022}, Oud-West, De Baarjes \cite{rettberg_when_2019}; and non-gentrified areas include Zuidoost and Nieuw-West \cite{pinkster_stickiness_2020}. Images taken from these neighborhoods are labeled accordingly. Thus, when testing the model (trained on visual perception) on this data (labeled per census data), we could tell to which extent the perception is applicable on a neighborhood level. In other words, given a gentrified neighborhood, to which extent are signage in that neighborhood seen as gentrified? Further, the model's predictions on this data were also visualized as per the highest 20 probabilities per class, irrespective of ground truth labels. This showed what the model considered as gentrified and non-gentrified, and were this to be in-line with characteristics found in StreetSwipe, this would support the model's generalizability in detecting gentrification.

